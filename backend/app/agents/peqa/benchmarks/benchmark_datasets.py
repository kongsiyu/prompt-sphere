"""
PEQAåŸºå‡†æµ‹è¯•æ•°æ®é›†

æä¾›æ ‡å‡†åŒ–çš„æµ‹è¯•æ•°æ®é›†ç”¨äºæ€§èƒ½ã€è´¨é‡ã€å‡†ç¡®æ€§åŸºå‡†æµ‹è¯•
"""

from typing import Dict, List, Any
from ..types import QualityDimension


class BenchmarkDatasets:
    """åŸºå‡†æµ‹è¯•æ•°æ®é›†"""

    @staticmethod
    def get_performance_test_prompts() -> List[str]:
        """æ€§èƒ½æµ‹è¯•æç¤ºè¯é›†åˆï¼ˆç”¨äºååé‡å’Œå»¶è¿Ÿæµ‹è¯•ï¼‰"""
        return [
            # çŸ­æç¤ºè¯ (< 50å­—ç¬¦)
            "å†™ä»£ç ",
            "åˆ†ææ•°æ®",
            "åˆ›å»ºæŠ¥å‘Š",
            "ä¼˜åŒ–æ€§èƒ½",
            "è®¾è®¡ç³»ç»Ÿ",

            # ä¸­ç­‰æç¤ºè¯ (50-200å­—ç¬¦)
            "è¯·ä¸ºæˆ‘ç¼–å†™ä¸€ä¸ªPythonå‡½æ•°ï¼Œç”¨äºè®¡ç®—ä¸¤ä¸ªæ•°å­—çš„å¹³å‡å€¼ã€‚",
            "åˆ†æè¿™ä»½é”€å”®æ•°æ®ï¼Œæ‰¾å‡ºå¢é•¿è¶‹åŠ¿å’Œå¼‚å¸¸å€¼ã€‚",
            "åˆ›å»ºä¸€ä¸ªè¯¦ç»†çš„é¡¹ç›®è¿›åº¦æŠ¥å‘Šï¼ŒåŒ…å«æ—¶é—´çº¿å’Œé‡Œç¨‹ç¢‘ã€‚",
            "ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ï¼Œå‡å°‘å“åº”æ—¶é—´åˆ°100msä»¥å†…ã€‚",
            "è®¾è®¡ä¸€ä¸ªå¾®æœåŠ¡æ¶æ„ï¼Œæ”¯æŒé«˜å¹¶å‘å’Œæ¨ªå‘æ‰©å±•ã€‚",

            # é•¿æç¤ºè¯ (200-500å­—ç¬¦)
            "ä½œä¸ºä¸€åèµ„æ·±çš„æ•°æ®ç§‘å­¦å®¶ï¼Œè¯·ä¸ºæˆ‘è®¾è®¡ä¸€ä¸ªå®Œæ•´çš„æœºå™¨å­¦ä¹ é¡¹ç›®æ–¹æ¡ˆã€‚é¡¹ç›®ç›®æ ‡æ˜¯é¢„æµ‹å®¢æˆ·æµå¤±ï¼Œéœ€è¦åŒ…å«æ•°æ®æ”¶é›†ã€é¢„å¤„ç†ã€ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹é€‰æ‹©ã€è®­ç»ƒã€è¯„ä¼°å’Œéƒ¨ç½²çš„å®Œæ•´æµç¨‹ã€‚è¯·æä¾›è¯¦ç»†çš„æŠ€æœ¯æ ˆé€‰æ‹©ç†ç”±å’Œå®æ–½æ—¶é—´è¡¨ã€‚",
            "è¯·å¸®æˆ‘åˆ›å»ºä¸€ä¸ªä¼ä¸šçº§çš„å®¢æˆ·å…³ç³»ç®¡ç†ç³»ç»Ÿè®¾è®¡æ–¹æ¡ˆã€‚ç³»ç»Ÿéœ€è¦æ”¯æŒé”€å”®æµç¨‹ç®¡ç†ã€å®¢æˆ·æ•°æ®åˆ†æã€è¥é”€è‡ªåŠ¨åŒ–å’ŒæŠ¥è¡¨ç”ŸæˆåŠŸèƒ½ã€‚è¦æ±‚ä½¿ç”¨å¾®æœåŠ¡æ¶æ„ï¼Œæ”¯æŒ10ä¸‡å¹¶å‘ç”¨æˆ·ï¼Œå¹¶ä¸”å…·å¤‡é«˜å¯ç”¨æ€§å’Œæ•°æ®å®‰å…¨ä¿éšœã€‚è¯·æä¾›è¯¦ç»†çš„æ¶æ„å›¾å’ŒæŠ€æœ¯é€‰å‹è¯´æ˜ã€‚",

            # å¤æ‚æç¤ºè¯ (500+å­—ç¬¦)
            "æˆ‘æ­£åœ¨å¼€å‘ä¸€ä¸ªæ™ºèƒ½æ¨èç³»ç»Ÿï¼Œç”¨äºç”µå•†å¹³å°çš„å•†å“æ¨èã€‚ç³»ç»Ÿéœ€è¦å¤„ç†ç”¨æˆ·è¡Œä¸ºæ•°æ®ï¼ˆæµè§ˆã€è´­ä¹°ã€è¯„ä»·ï¼‰ã€å•†å“å±æ€§æ•°æ®ï¼ˆç±»åˆ«ã€ä»·æ ¼ã€å“ç‰Œã€æè¿°ï¼‰å’Œå®æ—¶ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆæ—¶é—´ã€åœ°ç‚¹ã€è®¾å¤‡ï¼‰ã€‚è¯·ä¸ºæˆ‘è®¾è®¡ä¸€ä¸ªç»¼åˆçš„æ¨èç®—æ³•æ¡†æ¶ï¼ŒåŒ…æ‹¬ååŒè¿‡æ»¤ã€å†…å®¹æ¨èã€æ·±åº¦å­¦ä¹ æ¨¡å‹ç­‰å¤šç§æ–¹æ³•çš„èåˆã€‚åŒæ—¶éœ€è¦è€ƒè™‘å†·å¯åŠ¨é—®é¢˜ã€å®æ—¶æ€§è¦æ±‚ã€A/Bæµ‹è¯•æ¡†æ¶å’Œæ¨èç»“æœçš„å¯è§£é‡Šæ€§ã€‚è¯·æä¾›è¯¦ç»†çš„ç³»ç»Ÿæ¶æ„ã€ç®—æ³•é€‰æ‹©ã€æ•°æ®æµè®¾è®¡å’Œæ€§èƒ½ä¼˜åŒ–ç­–ç•¥ã€‚",
        ]

    @staticmethod
    def get_quality_test_cases() -> List[Dict[str, Any]]:
        """è´¨é‡æµ‹è¯•ç”¨ä¾‹ï¼ˆå¸¦é¢„æœŸè¯„åˆ†ï¼‰"""
        return [
            {
                "prompt": "è¯·ä¸ºæˆ‘åˆ›å»ºä¸€ä¸ªPythonå‡½æ•°ï¼Œè®¡ç®—åˆ—è¡¨ä¸­æ‰€æœ‰æ•°å­—çš„å¹³å‡å€¼ï¼Œè¦æ±‚å¤„ç†ç©ºåˆ—è¡¨å’Œéæ•°å­—å€¼çš„å¼‚å¸¸æƒ…å†µã€‚",
                "expected_score": 0.85,
                "expected_level": "good",
                "expected_dimensions": {
                    QualityDimension.CLARITY: 0.9,
                    QualityDimension.SPECIFICITY: 0.8,
                    QualityDimension.COMPLETENESS: 0.8,
                    QualityDimension.EFFECTIVENESS: 0.9,
                    QualityDimension.ROBUSTNESS: 0.8
                },
                "expected_issues": []
            },
            {
                "prompt": "å†™ä»£ç ",
                "expected_score": 0.15,
                "expected_level": "very_poor",
                "expected_dimensions": {
                    QualityDimension.CLARITY: 0.2,
                    QualityDimension.SPECIFICITY: 0.1,
                    QualityDimension.COMPLETENESS: 0.1,
                    QualityDimension.EFFECTIVENESS: 0.2,
                    QualityDimension.ROBUSTNESS: 0.1
                },
                "expected_issues": ["clarity", "specificity", "completeness"]
            },
            {
                "prompt": "åˆ†æé”€å”®æ•°æ®å¹¶ç”ŸæˆæŠ¥å‘Š",
                "expected_score": 0.45,
                "expected_level": "fair",
                "expected_dimensions": {
                    QualityDimension.CLARITY: 0.6,
                    QualityDimension.SPECIFICITY: 0.3,
                    QualityDimension.COMPLETENESS: 0.4,
                    QualityDimension.EFFECTIVENESS: 0.6,
                    QualityDimension.ROBUSTNESS: 0.3
                },
                "expected_issues": ["specificity", "completeness"]
            },
            {
                "prompt": "ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„Pythonå¼€å‘å·¥ç¨‹å¸ˆï¼Œè¯·ä¸ºæˆ‘ç¼–å†™ä¸€ä¸ªå®Œæ•´çš„æ•°æ®åˆ†æè„šæœ¬ã€‚è¦æ±‚ï¼š1)è¯»å–CSVæ–‡ä»¶data.csvï¼Œ2)è®¡ç®—é”€å”®é¢çš„å¹³å‡å€¼ã€ä¸­ä½æ•°å’Œæ ‡å‡†å·®ï¼Œ3)ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ä¿å­˜ä¸ºsales_analysis.pngï¼Œ4)è¾“å‡ºè¯¦ç»†çš„ç»Ÿè®¡æŠ¥å‘Šã€‚è¯·åŒ…å«é”™è¯¯å¤„ç†å’Œä»£ç æ³¨é‡Šã€‚",
                "expected_score": 0.92,
                "expected_level": "excellent",
                "expected_dimensions": {
                    QualityDimension.CLARITY: 0.95,
                    QualityDimension.SPECIFICITY: 0.9,
                    QualityDimension.COMPLETENESS: 0.9,
                    QualityDimension.EFFECTIVENESS: 0.95,
                    QualityDimension.ROBUSTNESS: 0.9
                },
                "expected_issues": []
            }
        ]

    @staticmethod
    def get_scalability_test_prompts() -> List[str]:
        """å¯æ‰©å±•æ€§æµ‹è¯•åŸºç¡€æç¤ºè¯"""
        return [
            "è¯·åˆ›å»ºä¸€ä¸ªWebåº”ç”¨",
            "åˆ†æç”¨æˆ·è¡Œä¸ºæ•°æ®ï¼Œæ‰¾å‡ºå…³é”®æ¨¡å¼",
            "è®¾è®¡æ•°æ®åº“schemaæ”¯æŒç”µå•†ç³»ç»Ÿ",
            "ä¼˜åŒ–APIæ€§èƒ½æå‡å“åº”é€Ÿåº¦",
            "å®ç°ç”¨æˆ·è®¤è¯å’Œæƒé™ç®¡ç†ç³»ç»Ÿ"
        ]

    @staticmethod
    def get_accuracy_gold_standard() -> List[Dict[str, Any]]:
        """å‡†ç¡®æ€§æµ‹è¯•é»„é‡‘æ ‡å‡†æ•°æ®é›†"""
        return [
            {
                "prompt": "è¯·å¸®æˆ‘å†™ä¸€ä¸ªhello worldç¨‹åº",
                "ground_truth": {
                    "overall_score": 0.3,
                    "quality_level": "poor",
                    "dimension_scores": {
                        "clarity": 0.6,
                        "specificity": 0.2,
                        "completeness": 0.2,
                        "effectiveness": 0.4,
                        "robustness": 0.2
                    },
                    "expected_issues": ["specificity", "completeness"],
                    "improvement_categories": ["specificity", "completeness"]
                }
            },
            {
                "prompt": "ä½œä¸ºä¸€åç»éªŒä¸°å¯Œçš„è½¯ä»¶æ¶æ„å¸ˆï¼Œè¯·ä¸ºæˆ‘è®¾è®¡ä¸€ä¸ªé«˜æ€§èƒ½çš„åˆ†å¸ƒå¼ç¼“å­˜ç³»ç»Ÿã€‚ç³»ç»Ÿéœ€è¦æ”¯æŒï¼š1)æ•°æ®åˆ†ç‰‡å’Œè´Ÿè½½å‡è¡¡ï¼Œ2)æ•…éšœæ£€æµ‹å’Œè‡ªåŠ¨æ¢å¤ï¼Œ3)æ•°æ®ä¸€è‡´æ€§ä¿è¯ï¼Œ4)ç›‘æ§å’Œå‘Šè­¦åŠŸèƒ½ã€‚è¯·æä¾›è¯¦ç»†çš„æ¶æ„è®¾è®¡ã€æŠ€æœ¯é€‰å‹ç†ç”±ã€éƒ¨ç½²æ–¹æ¡ˆå’Œæ€§èƒ½ä¼˜åŒ–ç­–ç•¥ã€‚",
                "ground_truth": {
                    "overall_score": 0.88,
                    "quality_level": "good",
                    "dimension_scores": {
                        "clarity": 0.9,
                        "specificity": 0.85,
                        "completeness": 0.9,
                        "effectiveness": 0.9,
                        "robustness": 0.85
                    },
                    "expected_issues": [],
                    "improvement_categories": []
                }
            },
            {
                "prompt": "ä¼˜åŒ–ä»£ç æ€§èƒ½",
                "ground_truth": {
                    "overall_score": 0.2,
                    "quality_level": "very_poor",
                    "dimension_scores": {
                        "clarity": 0.3,
                        "specificity": 0.1,
                        "completeness": 0.1,
                        "effectiveness": 0.2,
                        "robustness": 0.1
                    },
                    "expected_issues": ["clarity", "specificity", "completeness", "effectiveness"],
                    "improvement_categories": ["clarity", "specificity", "completeness"]
                }
            }
        ]

    @staticmethod
    def get_stress_test_prompts(count: int = 1000) -> List[str]:
        """å‹åŠ›æµ‹è¯•æç¤ºè¯ç”Ÿæˆå™¨"""
        base_prompts = BenchmarkDatasets.get_performance_test_prompts()

        # æ‰©å±•åˆ°æŒ‡å®šæ•°é‡
        prompts = []
        for i in range(count):
            base_prompt = base_prompts[i % len(base_prompts)]
            # æ·»åŠ å˜åŒ–ä½¿æ¯ä¸ªæç¤ºè¯ç•¥æœ‰ä¸åŒ
            varied_prompt = f"{base_prompt} (æµ‹è¯•ç¼–å·: {i+1})"
            prompts.append(varied_prompt)

        return prompts

    @staticmethod
    def get_domain_specific_prompts() -> Dict[str, List[str]]:
        """é¢†åŸŸç‰¹å®šæµ‹è¯•æç¤ºè¯"""
        return {
            "programming": [
                "ç¼–å†™ä¸€ä¸ªPythonçˆ¬è™«æŠ“å–æ–°é—»æ ‡é¢˜",
                "å®ç°äºŒåˆ†æŸ¥æ‰¾ç®—æ³•å¹¶åˆ†ææ—¶é—´å¤æ‚åº¦",
                "è®¾è®¡RESTful APIæ¥å£è§„èŒƒ",
                "åˆ›å»ºReactç»„ä»¶å¤„ç†è¡¨å•éªŒè¯",
                "ä¼˜åŒ–SQLæŸ¥è¯¢æå‡æ•°æ®åº“æ€§èƒ½"
            ],
            "data_analysis": [
                "åˆ†æç”¨æˆ·ç•™å­˜ç‡å˜åŒ–è¶‹åŠ¿",
                "æ„å»ºé”€å”®é¢„æµ‹æ¨¡å‹",
                "è®¾è®¡A/Bæµ‹è¯•ç»Ÿè®¡åˆ†æ",
                "åˆ›å»ºæ•°æ®å¯è§†åŒ–ä»ªè¡¨æ¿",
                "å®ç°å®æ—¶æ•°æ®æµå¤„ç†"
            ],
            "business": [
                "åˆ¶å®šäº§å“è¥é”€ç­–ç•¥",
                "åˆ†æç«äº‰å¯¹æ‰‹ä¼˜åŠ£åŠ¿",
                "è®¾è®¡å®¢æˆ·æ»¡æ„åº¦è°ƒç ”æ–¹æ¡ˆ",
                "ä¼˜åŒ–ä¸šåŠ¡æµç¨‹æå‡æ•ˆç‡",
                "åˆ¶å®šæ•°å­—åŒ–è½¬å‹è®¡åˆ’"
            ],
            "creative": [
                "å†™ä¸€ä¸ªç§‘å¹»å°è¯´å¼€å¤´",
                "è®¾è®¡å“ç‰Œlogoå’Œè§†è§‰è¯†åˆ«",
                "åˆ›ä½œå¹¿å‘Šæ–‡æ¡ˆå¸å¼•ç›®æ ‡å®¢æˆ·",
                "ç¼–å†™æŠ€æœ¯æ–‡æ¡£å’Œä½¿ç”¨æ‰‹å†Œ",
                "è®¾è®¡ç”¨æˆ·ä½“éªŒæµç¨‹"
            ]
        }

    @staticmethod
    def get_edge_case_prompts() -> List[str]:
        """è¾¹ç•Œæƒ…å†µæµ‹è¯•æç¤ºè¯"""
        return [
            "",  # ç©ºæç¤ºè¯
            " ",  # åªæœ‰ç©ºæ ¼
            "a",  # å•å­—ç¬¦
            "?" * 100,  # é‡å¤å­—ç¬¦
            "å†™" + "ä¸€ä¸ªå¾ˆé•¿çš„" * 50 + "ç¨‹åº",  # è¶…é•¿æç¤ºè¯
            "Write code in English mixed with ä¸­æ–‡ and special chars !@#$%",  # æ··åˆè¯­è¨€å’Œç‰¹æ®Šå­—ç¬¦
            "\n\n\nå¤šè¡Œ\n\næç¤ºè¯\n\næµ‹è¯•\n\n",  # å¤šæ¢è¡Œç¬¦
            "ğŸš€ğŸ”¥ğŸ’¡ğŸ“ŠğŸ¯" * 20,  # å¤§é‡emoji
        ]

    @staticmethod
    def generate_synthetic_dataset(size: int, complexity_distribution: Dict[str, float] = None) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆåˆæˆæµ‹è¯•æ•°æ®é›†

        Args:
            size: æ•°æ®é›†å¤§å°
            complexity_distribution: å¤æ‚åº¦åˆ†å¸ƒ {"simple": 0.3, "medium": 0.5, "complex": 0.2}
        """
        if complexity_distribution is None:
            complexity_distribution = {"simple": 0.4, "medium": 0.4, "complex": 0.2}

        datasets = []
        simple_templates = [
            "åˆ›å»º{object}",
            "åˆ†æ{data}",
            "ä¼˜åŒ–{target}",
            "è®¾è®¡{system}",
            "å®ç°{feature}"
        ]

        medium_templates = [
            "è¯·ä¸ºæˆ‘{action}ä¸€ä¸ª{object}ï¼Œè¦æ±‚{requirement}",
            "ä½œä¸º{role}ï¼Œå¸®æˆ‘{action}{target}ï¼Œéœ€è¦è€ƒè™‘{constraint}",
            "åˆ†æ{data}å¹¶ç”Ÿæˆ{output}ï¼ŒåŒ…å«{details}"
        ]

        complex_templates = [
            "ä½œä¸ºä¸€åä¸“ä¸šçš„{expert}ï¼Œè¯·ä¸ºæˆ‘è®¾è®¡ä¸€ä¸ª{system}ã€‚è¦æ±‚ï¼š1){req1}ï¼Œ2){req2}ï¼Œ3){req3}ã€‚è¯·æä¾›è¯¦ç»†çš„{deliverable}å’Œ{timeline}ã€‚",
            "æˆ‘éœ€è¦å®ç°ä¸€ä¸ª{complex_system}ï¼Œå®ƒåº”è¯¥å…·å¤‡{feature1}ã€{feature2}å’Œ{feature3}åŠŸèƒ½ã€‚è¯·è€ƒè™‘{constraint1}ã€{constraint2}çš„é™åˆ¶ï¼Œå¹¶æä¾›{solution_type}å’Œ{implementation_details}ã€‚"
        ]

        # è®¡ç®—å„å¤æ‚åº¦çš„æ•°é‡
        simple_count = int(size * complexity_distribution["simple"])
        medium_count = int(size * complexity_distribution["medium"])
        complex_count = size - simple_count - medium_count

        # ç”Ÿæˆä¸åŒå¤æ‚åº¦çš„æç¤ºè¯
        for i in range(simple_count):
            template = simple_templates[i % len(simple_templates)]
            prompt = template.format(
                object=f"å·¥å…·{i}",
                data=f"æ•°æ®{i}",
                target=f"ç›®æ ‡{i}",
                system=f"ç³»ç»Ÿ{i}",
                feature=f"åŠŸèƒ½{i}"
            )
            datasets.append({
                "prompt": prompt,
                "complexity": "simple",
                "expected_score_range": (0.2, 0.5)
            })

        for i in range(medium_count):
            template = medium_templates[i % len(medium_templates)]
            prompt = template.format(
                action=f"åˆ›å»º",
                object=f"åº”ç”¨{i}",
                requirement=f"é«˜æ€§èƒ½å’Œå¯æ‰©å±•æ€§",
                role=f"å¼€å‘å·¥ç¨‹å¸ˆ",
                target=f"ç³»ç»Ÿ{i}",
                constraint=f"èµ„æºé™åˆ¶",
                data=f"ç”¨æˆ·æ•°æ®",
                output=f"æŠ¥å‘Š",
                details=f"è¶‹åŠ¿åˆ†æ"
            )
            datasets.append({
                "prompt": prompt,
                "complexity": "medium",
                "expected_score_range": (0.4, 0.7)
            })

        for i in range(complex_count):
            template = complex_templates[i % len(complex_templates)]
            prompt = template.format(
                expert=f"ç³»ç»Ÿæ¶æ„å¸ˆ",
                system=f"åˆ†å¸ƒå¼ç³»ç»Ÿ{i}",
                req1=f"é«˜å¹¶å‘å¤„ç†",
                req2=f"æ•°æ®ä¸€è‡´æ€§",
                req3=f"æ•…éšœæ¢å¤",
                deliverable=f"æ¶æ„è®¾è®¡",
                timeline=f"å®æ–½è®¡åˆ’",
                complex_system=f"æ™ºèƒ½æ¨èç³»ç»Ÿ{i}",
                feature1=f"å®æ—¶è®¡ç®—",
                feature2=f"æœºå™¨å­¦ä¹ ",
                feature3=f"ä¸ªæ€§åŒ–æ¨è",
                constraint1=f"å»¶è¿Ÿè¦æ±‚",
                constraint2=f"æˆæœ¬æ§åˆ¶",
                solution_type=f"æŠ€æœ¯æ–¹æ¡ˆ",
                implementation_details=f"å®ç°ç»†èŠ‚"
            )
            datasets.append({
                "prompt": prompt,
                "complexity": "complex",
                "expected_score_range": (0.6, 0.9)
            })

        return datasets