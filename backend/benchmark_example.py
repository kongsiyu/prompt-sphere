#!/usr/bin/env python3
"""
PEQAåŸºå‡†æµ‹è¯•ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨PEQAçš„åŸºå‡†æµ‹è¯•åŠŸèƒ½
"""

import sys
import os
import asyncio
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'app'))

async def run_basic_benchmark():
    """è¿è¡ŒåŸºæœ¬åŸºå‡†æµ‹è¯•"""
    try:
        print("=== PEQAåŸºå‡†æµ‹è¯•ç¤ºä¾‹ ===\n")

        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        from agents.peqa.PEQAAgent import PEQAAgent
        from agents.peqa.config import PEQAConfig
        from agents.peqa.BenchmarkRunner import BenchmarkRunner, BenchmarkConfig
        from agents.peqa.benchmarks.benchmark_datasets import BenchmarkDatasets
        from agents.peqa.types import BenchmarkType

        # åˆ›å»ºPEQA Agent
        config = PEQAConfig()
        config.enable_parallel_processing = True
        config.batch_size = 3

        agent = PEQAAgent(config)
        benchmark_runner = BenchmarkRunner(agent, config)

        print("âœ… PEQA Agentå’ŒåŸºå‡†æµ‹è¯•è¿è¡Œå™¨åˆ›å»ºæˆåŠŸ\n")

        # 1. æ€§èƒ½åŸºå‡†æµ‹è¯•
        print("ğŸš€ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        performance_prompts = BenchmarkDatasets.get_performance_test_prompts()[:10]  # ä½¿ç”¨å‰10ä¸ª

        benchmark_config = BenchmarkConfig(
            test_type=BenchmarkType.PERFORMANCE,
            parallel_workers=3,
            benchmark_rounds=2,
            warmup_rounds=1
        )

        performance_result = await benchmark_runner.run_performance_benchmark(
            performance_prompts, benchmark_config
        )

        print(f"âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ:")
        print(f"   - æµ‹è¯•æ ·æœ¬: {performance_result.total_prompts}")
        print(f"   - å¹³å‡ååé‡: {performance_result.throughput:.2f} prompts/sec")
        print(f"   - å¹³å‡å¤„ç†æ—¶é—´: {performance_result.average_processing_time_ms:.2f}ms")
        print(f"   - æ€»å¤„ç†æ—¶é—´: {performance_result.total_processing_time_ms}ms")

        if performance_result.performance_metrics:
            print(f"   - å†…å­˜ä½¿ç”¨: {performance_result.performance_metrics.get('memory_used_mb', 0):.2f}MB")
            print(f"   - å¹¶è¡Œå·¥ä½œè€…: {performance_result.performance_metrics.get('parallel_workers', 0)}")

        print()

        # 2. è´¨é‡åŸºå‡†æµ‹è¯•
        print("ğŸ“Š å¼€å§‹è´¨é‡åŸºå‡†æµ‹è¯•...")
        quality_test_cases = BenchmarkDatasets.get_quality_test_cases()

        quality_result = await benchmark_runner.run_quality_benchmark(quality_test_cases)

        print(f"âœ… è´¨é‡æµ‹è¯•å®Œæˆ:")
        print(f"   - æµ‹è¯•ç”¨ä¾‹: {quality_result.total_prompts}")
        print(f"   - å¹³å‡è¯„åˆ†: {quality_result.average_score:.3f}")
        print(f"   - æœ€é«˜è¯„åˆ†: {quality_result.highest_score:.3f}")
        print(f"   - æœ€ä½è¯„åˆ†: {quality_result.lowest_score:.3f}")

        if quality_result.performance_metrics:
            accuracy = quality_result.performance_metrics.get('assessment_accuracy', 0)
            print(f"   - è¯„ä¼°å‡†ç¡®æ€§: {accuracy:.3f}")

        print()

        # 3. å¯æ‰©å±•æ€§åŸºå‡†æµ‹è¯•
        print("ğŸ“ˆ å¼€å§‹å¯æ‰©å±•æ€§åŸºå‡†æµ‹è¯•...")
        base_prompts = BenchmarkDatasets.get_scalability_test_prompts()
        scale_factors = [1, 2, 3]  # æµ‹è¯•1x, 2x, 3xè§„æ¨¡

        scalability_results = await benchmark_runner.run_scalability_benchmark(
            base_prompts, scale_factors
        )

        print(f"âœ… å¯æ‰©å±•æ€§æµ‹è¯•å®Œæˆ:")
        for scale_name, result in scalability_results.items():
            scale_factor = result.performance_metrics['scale_factor']
            efficiency = result.performance_metrics.get('efficiency_ratio', 0)
            print(f"   - {scale_name}: ååé‡ {result.throughput:.2f}, æ•ˆç‡æ¯” {efficiency:.3f}")

        print()

        # 4. å‡†ç¡®æ€§åŸºå‡†æµ‹è¯•
        print("ğŸ¯ å¼€å§‹å‡†ç¡®æ€§åŸºå‡†æµ‹è¯•...")
        gold_standard = BenchmarkDatasets.get_accuracy_gold_standard()

        accuracy_result = await benchmark_runner.run_accuracy_benchmark(gold_standard)

        print(f"âœ… å‡†ç¡®æ€§æµ‹è¯•å®Œæˆ:")
        print(f"   - é»„é‡‘æ ‡å‡†æ ·æœ¬: {accuracy_result.total_prompts}")
        print(f"   - å¹³å‡è¯„åˆ†: {accuracy_result.average_score:.3f}")

        if accuracy_result.performance_metrics:
            mae = accuracy_result.performance_metrics.get('overall_score_mae', 0)
            level_accuracy = accuracy_result.performance_metrics.get('quality_level_accuracy', 0)
            print(f"   - è¯„åˆ†è¯¯å·®(MAE): {mae:.3f}")
            print(f"   - ç­‰çº§å‡†ç¡®ç‡: {level_accuracy:.3f}")

        print()

        # 5. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        print("ğŸ“ ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š...")
        all_results = {
            "æ€§èƒ½æµ‹è¯•": performance_result,
            "è´¨é‡æµ‹è¯•": quality_result,
            "å‡†ç¡®æ€§æµ‹è¯•": accuracy_result,
            **{f"å¯æ‰©å±•æ€§æµ‹è¯•_{k}": v for k, v in scalability_results.items()}
        }

        report = await benchmark_runner.generate_benchmark_report(all_results)

        # ä¿å­˜æŠ¥å‘Š
        report_file = f"peqa_benchmark_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

        # 6. æ˜¾ç¤ºæŠ¥å‘Šæ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ“Š åŸºå‡†æµ‹è¯•æ‘˜è¦")
        print("="*60)
        print(f"æ€§èƒ½ååé‡: {performance_result.throughput:.2f} prompts/sec")
        print(f"è´¨é‡è¯„ä¼°å‡†ç¡®æ€§: {quality_result.performance_metrics.get('assessment_accuracy', 0):.3f}")
        print(f"ç³»ç»Ÿå“åº”æ—¶é—´: {performance_result.average_processing_time_ms:.2f}ms")
        print(f"å¯æ‰©å±•æ€§æ•ˆç‡: {scalability_results.get('scale_3x', scalability_results[list(scalability_results.keys())[-1]]).performance_metrics.get('efficiency_ratio', 0):.3f}")

        return True

    except Exception as e:
        print(f"âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

async def run_stress_test():
    """è¿è¡Œå‹åŠ›æµ‹è¯•"""
    try:
        print("\nğŸ”¥ å¼€å§‹å‹åŠ›æµ‹è¯•...")

        from agents.peqa.PEQAAgent import PEQAAgent
        from agents.peqa.config import PEQAConfig
        from agents.peqa.benchmarks.benchmark_datasets import BenchmarkDatasets

        # åˆ›å»ºé«˜å¹¶å‘é…ç½®
        config = PEQAConfig()
        config.enable_parallel_processing = True
        config.batch_size = 10

        agent = PEQAAgent(config)

        # ç”Ÿæˆå¤§é‡æµ‹è¯•æ•°æ®
        stress_prompts = BenchmarkDatasets.get_stress_test_prompts(50)  # 50ä¸ªæç¤ºè¯

        print(f"ç”Ÿæˆ {len(stress_prompts)} ä¸ªæµ‹è¯•æç¤ºè¯")

        start_time = datetime.now()

        # æ‰§è¡Œå‹åŠ›æµ‹è¯•
        results = await agent.benchmark_performance(stress_prompts)

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        print(f"âœ… å‹åŠ›æµ‹è¯•å®Œæˆ:")
        print(f"   - å¤„ç†æ—¶é—´: {duration:.2f}ç§’")
        print(f"   - ååé‡: {results.throughput:.2f} prompts/sec")
        print(f"   - æˆåŠŸç‡: {results.performance_metrics.get('success_rate', 0):.3f}")
        print(f"   - é”™è¯¯æ•°é‡: {results.performance_metrics.get('error_count', 0)}")

        return True

    except Exception as e:
        print(f"âŒ å‹åŠ›æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

async def run_edge_case_test():
    """è¿è¡Œè¾¹ç•Œæƒ…å†µæµ‹è¯•"""
    try:
        print("\nğŸ§ª å¼€å§‹è¾¹ç•Œæƒ…å†µæµ‹è¯•...")

        from agents.peqa.PEQAAgent import PEQAAgent
        from agents.peqa.config import PEQAConfig
        from agents.peqa.benchmarks.benchmark_datasets import BenchmarkDatasets

        config = PEQAConfig()
        agent = PEQAAgent(config)

        edge_cases = BenchmarkDatasets.get_edge_case_prompts()

        print(f"æµ‹è¯• {len(edge_cases)} ä¸ªè¾¹ç•Œæƒ…å†µ")

        success_count = 0
        error_count = 0

        for i, prompt in enumerate(edge_cases):
            try:
                assessment = await agent.assess_prompt(prompt)
                success_count += 1
                print(f"âœ… è¾¹ç•Œæƒ…å†µ {i+1}: è¯„åˆ† {assessment.overall_score:.3f}")
            except Exception as e:
                error_count += 1
                print(f"âŒ è¾¹ç•Œæƒ…å†µ {i+1}: {str(e)[:50]}...")

        print(f"\nè¾¹ç•Œæƒ…å†µæµ‹è¯•ç»“æœ:")
        print(f"   - æˆåŠŸ: {success_count}/{len(edge_cases)}")
        print(f"   - å¤±è´¥: {error_count}/{len(edge_cases)}")
        print(f"   - æˆåŠŸç‡: {success_count/len(edge_cases):.3f}")

        return True

    except Exception as e:
        print(f"âŒ è¾¹ç•Œæƒ…å†µæµ‹è¯•å¤±è´¥: {str(e)}")
        return False

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨PEQAå®Œæ•´åŸºå‡†æµ‹è¯•å¥—ä»¶\n")

    # è¿è¡ŒåŸºæœ¬åŸºå‡†æµ‹è¯•
    basic_success = await run_basic_benchmark()

    # è¿è¡Œå‹åŠ›æµ‹è¯•
    stress_success = await run_stress_test()

    # è¿è¡Œè¾¹ç•Œæƒ…å†µæµ‹è¯•
    edge_success = await run_edge_case_test()

    # æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ¯ åŸºå‡†æµ‹è¯•æ€»ç»“")
    print("="*60)
    print(f"åŸºæœ¬åŸºå‡†æµ‹è¯•: {'âœ… é€šè¿‡' if basic_success else 'âŒ å¤±è´¥'}")
    print(f"å‹åŠ›æµ‹è¯•: {'âœ… é€šè¿‡' if stress_success else 'âŒ å¤±è´¥'}")
    print(f"è¾¹ç•Œæƒ…å†µæµ‹è¯•: {'âœ… é€šè¿‡' if edge_success else 'âŒ å¤±è´¥'}")

    overall_success = all([basic_success, stress_success, edge_success])
    print(f"\nğŸ† æ•´ä½“ç»“æœ: {'âœ… å…¨éƒ¨é€šè¿‡' if overall_success else 'âŒ éƒ¨åˆ†å¤±è´¥'}")

    return overall_success

if __name__ == "__main__":
    # è¿è¡Œå®Œæ•´åŸºå‡†æµ‹è¯•
    success = asyncio.run(main())
    sys.exit(0 if success else 1)