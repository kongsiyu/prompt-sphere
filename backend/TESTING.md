# Backend Testing Guide

## ğŸ§ª Overview

This document describes the pytest-based testing infrastructure for the AI Prompt Generator backend.

## ğŸ“ Test Structure

```
tests/
â”œâ”€â”€ conftest.py                    # Pytest configuration and fixtures
â”œâ”€â”€ test_models.py                 # Database model unit tests (22 tests)
â”œâ”€â”€ test_config.py                 # Configuration unit tests (18 tests)
â”œâ”€â”€ test_api.py                    # FastAPI application tests (20 tests)
â”œâ”€â”€ test_api_models.py             # Pydantic model tests (20 tests)
â”œâ”€â”€ test_api_endpoints.py          # API endpoint integration tests (23 tests)
â”œâ”€â”€ test_repositories.py           # Repository layer integration tests (@database)
â”œâ”€â”€ test_database_integration.py   # Database connection and session tests (@database)
â””â”€â”€ fixtures/                      # Test fixtures (if needed)
```

## ğŸ”§ Test Configuration

### Pytest Configuration

The project uses `pyproject.toml` for pytest configuration:

```toml
[tool.pytest.ini_options]
minversion = "7.0"
addopts = "-ra -q --strict-markers --strict-config"
testpaths = ["tests"]
asyncio_mode = "auto"
python_files = ["test_*.py", "*_test.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
```

### Test Markers

- `@pytest.mark.unit` - Unit tests (isolated, no external dependencies)
- `@pytest.mark.api` - API-related tests
- `@pytest.mark.database` - Database tests (skipped by default)
- `@pytest.mark.slow` - Slow-running tests

## ğŸš€ Running Tests

### Using the Custom Test Runner

```bash
# Run unit tests only (no database)
python run_pytest.py unit

# Run all unit tests (default)
python run_pytest.py all

# Run database integration tests only
python run_pytest.py database
# or
python run_pytest.py db

# Run complete test suite (unit + database)
python run_pytest.py full

# Run with detailed output
python run_pytest.py coverage
```

### Using pytest directly

```bash
# Run unit tests only
python -m pytest tests/test_models.py tests/test_config.py tests/test_api.py tests/test_api_models.py tests/test_api_endpoints.py -v

# Run database tests only
python -m pytest tests/test_repositories.py tests/test_database_integration.py -v -m database --run-database-tests

# Run all tests
python -m pytest tests/ -v

# Run only unit tests
python -m pytest -m unit -v

# Run with coverage
python -m pytest --cov=app --cov=database.models --cov-report=term-missing
```

## ğŸ“Š Test Coverage

### ğŸ”¹ Unit Tests (Always Run)

#### Database Models (`test_models.py`)
- âœ… User model creation and validation
- âœ… Template model with JSON fields
- âœ… Conversation model with context/settings
- âœ… Prompt model with sequence management
- âœ… AuditLog model with metadata
- âœ… Model inheritance and table structure

#### Configuration (`test_config.py`)
- âœ… Settings instantiation and field access
- âœ… Database configuration validation
- âœ… API settings and CORS configuration
- âœ… Security settings validation
- âœ… Field type checking and defaults

#### API Layer
**Application Structure** (`test_api.py`)
- âœ… FastAPI application structure
- âœ… Basic endpoint existence
- âœ… Request/response handling
- âœ… Error handling behavior
- âœ… Content type management

**Pydantic Models** (`test_api_models.py`)
- âœ… PromptRequest validation and serialization
- âœ… PromptResponse structure and defaults
- âœ… HealthResponse model validation
- âœ… Field type validation and error handling
- âœ… Model integration workflows

**API Endpoints** (`test_api_endpoints.py`)
- âœ… Health check endpoint functionality
- âœ… Prompt generation with all field combinations
- âœ… Request validation and error responses
- âœ… Unicode and special character handling
- âœ… API documentation endpoints

### ğŸ”¹ Database Integration Tests (Optional)

#### Repository Layer (`test_repositories.py`)
- âœ… UserRepository CRUD operations
- âœ… TemplateRepository with category and tag search
- âœ… ConversationRepository with context handling
- âœ… PromptRepository with sequence management
- âœ… AuditLogRepository with action tracking
- âœ… Cross-repository transactions
- âœ… Error handling and constraint validation

#### Database Integration (`test_database_integration.py`)
- âœ… Basic connection and session management
- âœ… Transaction commit and rollback behavior
- âœ… Concurrent session handling
- âœ… Long-running session stability
- âœ… Error handling and recovery
- âœ… Schema operations and constraints
- âœ… Index creation and usage

## ğŸ—ï¸ Test Fixtures

The `conftest.py` provides comprehensive fixtures:

```python
# Mock database sessions
mock_database_session()
mock_engine()

# Sample data fixtures
sample_user_data()
sample_template_data()
sample_conversation_data()
sample_prompt_data()

# Model instance fixtures
sample_user()
sample_template()
sample_conversation()
sample_prompt()

# API client
client()  # FastAPI TestClient
```

## ğŸ¯ Test Strategy

### Unit Tests Philosophy
- **Isolated**: Tests don't depend on external services
- **Fast**: All tests complete in under 1 second
- **Reliable**: Tests pass consistently without flakiness
- **Focused**: Each test validates a specific behavior

### Database Test Strategy
- Models are tested without database connections
- Use mock sessions for repository testing
- Validate field types and constraints
- Test JSON field serialization

### API Test Strategy
- Test application structure and configuration
- Validate endpoint existence and basic behavior
- Test error handling and edge cases
- Avoid database dependencies in unit tests

## ğŸ“ˆ Test Results

Current test status:
- **103 passing tests** âœ…
- **0 failing tests** âœ…
- **35 warnings** (Pydantic/SQLAlchemy deprecations)
- **~0.32s** execution time

## ğŸ› ï¸ Development Workflow

### Adding New Tests

1. Create test files following the naming convention `test_*.py`
2. Use appropriate markers (`@pytest.mark.unit`, `@pytest.mark.api`)
3. Utilize existing fixtures from `conftest.py`
4. Run tests locally before committing

### Test Categories

#### Unit Tests âœ… (103 tests)
- Database models (without DB connection)
- Configuration classes
- API structure and endpoints
- Pydantic model validation
- Request/response handling

#### Database Integration Tests âœ… (Available)
- Repository CRUD operations
- Database connection management
- Transaction handling
- Concurrent access patterns
- Schema operations
- Error recovery scenarios

#### Performance Tests â¸ï¸ (Future)
- Load testing
- Memory usage profiling
- Connection pool optimization
- Query performance analysis

## ğŸš¨ Known Limitations

1. **Database Tests Optional**: Database integration tests require explicit activation with `--run-database-tests` flag
2. **External APIs**: No tests for OpenAI/Anthropic integrations yet
3. **Authentication**: JWT/auth testing not implemented
4. **E2E Testing**: No browser-based testing implemented

## ğŸ”® Future Improvements

1. **Performance Testing**: Add load and performance tests
2. **E2E Testing**: Browser-based testing with Playwright
3. **Contract Testing**: API contract validation
4. **External API Testing**: Mock OpenAI/Anthropic integrations
5. **Security Testing**: Authentication and authorization tests

## ğŸƒâ€â™‚ï¸ Quick Start

1. Install dependencies:
```bash
pip install -r requirements.txt
```

2. Run unit tests:
```bash
python run_pytest.py all
```

3. Run database tests (optional):
```bash
python run_pytest.py database
```

4. Run complete suite:
```bash
python run_pytest.py full
```

5. Verify unit test output:
```
ğŸ§ª Running All Available Tests with pytest
==================================================
tests\test_models.py ......................              [ 21%]
tests\test_config.py ..................                  [ 38%]
tests\test_api.py ....................                   [ 58%]
tests\test_api_models.py ....................            [ 77%]
tests\test_api_endpoints.py .......................        [100%]

======================= 103 passed, 35 warnings in 0.37s ========================
ğŸ‰ All tests passed!
```

## ğŸ“ Contributing

When adding new features:

1. Write tests first (TDD approach)
2. Ensure all tests pass
3. Maintain test coverage above 80%
4. Update this documentation

---

**Status**: âœ… Production Ready
**Last Updated**: 2024-01-01
**Unit Test Count**: 103 (always run)
**Database Test Count**: Available (run on demand)
**Coverage**: Complete unit testing + optional database integration testing